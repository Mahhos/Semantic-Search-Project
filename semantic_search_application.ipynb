{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and Preprocess Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "url_examples = 'https://github.com/amazon-science/esci-data/raw/main/shopping_queries_dataset/shopping_queries_dataset_examples.parquet'\n",
    "url_products = 'https://github.com/amazon-science/esci-data/raw/main/shopping_queries_dataset/shopping_queries_dataset_products.parquet'\n",
    "\n",
    "df_examples = pd.read_parquet(url_examples)\n",
    "df_products = pd.read_parquet(url_products)\n",
    "\n",
    "df_examples_products = pd.merge(\n",
    "    df_examples,\n",
    "    df_products,\n",
    "    how='left',\n",
    "    left_on=['product_locale', 'product_id'],\n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "\n",
    "df_task_1 = df_examples_products[df_examples_products[\"small_version\"] == 1]\n",
    "df_task_1_train = df_task_1[df_task_1[\"split\"] == \"train\"]\n",
    "df_task_1_test = df_task_1[df_task_1[\"split\"] == \"test\"]\n",
    "\n",
    "# Use only 10% of the data\n",
    "df_train = df_task_1_train.sample(frac=1, random_state=42)\n",
    "df_test = df_task_1_test.sample(frac=1, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {df_train.shape}\")\n",
    "print(f\"Testing data shape: {df_test.shape}\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Embeddings and FAISS Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding function\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def create_embeddings(texts):\n",
    "    return model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Create product embeddings\n",
    "print(\"Creating product embeddings...\")\n",
    "product_texts = df_train['product_title'] + ' ' + df_train['product_description'].fillna('')\n",
    "product_embeddings = create_embeddings(product_texts.tolist())\n",
    "\n",
    "# Create FAISS index\n",
    "print(\"Creating FAISS index...\")\n",
    "dimension = product_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(product_embeddings.astype('float32'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Search and Evaluation Functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search function\n",
    "def search(query, k=10):\n",
    "    query_embedding = create_embeddings([query])\n",
    "    D, I = index.search(query_embedding.astype('float32'), k)\n",
    "    return I[0]\n",
    "\n",
    "# Evaluation metrics\n",
    "def calculate_hits_at_n(predictions, actual, n):\n",
    "    return 1.0 if actual in predictions[:n] else 0.0\n",
    "\n",
    "def calculate_mrr(predictions, actual):\n",
    "    try:\n",
    "        rank = predictions.index(actual) + 1\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "# Evaluate search performance\n",
    "def evaluate_search(df, n_values):\n",
    "    hits_at_n = {n: [] for n in n_values}\n",
    "    mrr_scores = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        query = row['query']\n",
    "        actual_product_id = row['product_id']\n",
    "        predictions = search(query, max(n_values))\n",
    "        predicted_product_ids = df_train.iloc[predictions]['product_id'].tolist()\n",
    "\n",
    "        for n in n_values:\n",
    "            hits_at_n[n].append(calculate_hits_at_n(predicted_product_ids, actual_product_id, n))\n",
    "\n",
    "        mrr_scores.append(calculate_mrr(predicted_product_ids, actual_product_id))\n",
    "\n",
    "    hits_at_n_avg = {n: np.mean(scores) for n, scores in hits_at_n.items()}\n",
    "    mrr_avg = np.mean(mrr_scores)\n",
    "\n",
    "    return hits_at_n_avg, mrr_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement TF-IDF Ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondary ranking logic using TF-IDF\n",
    "print(\"Creating TF-IDF vectorizer...\")\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(product_texts)\n",
    "\n",
    "def secondary_ranking(query, initial_results, k=10):\n",
    "    query_vector = tfidf.transform([query])\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix[initial_results]).flatten()\n",
    "    reranked_indices = np.argsort(similarities)[::-1][:k]\n",
    "    return [initial_results[i] for i in reranked_indices]\n",
    "\n",
    "# Evaluate with secondary ranking\n",
    "def evaluate_search_with_secondary(df, n_values):\n",
    "    hits_at_n = {n: [] for n in n_values}\n",
    "    mrr_scores = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        query = row['query']\n",
    "        actual_product_id = row['product_id']\n",
    "        initial_results = search(query, max(n_values) * 2)\n",
    "        predictions = secondary_ranking(query, initial_results, max(n_values))\n",
    "        predicted_product_ids = df_train.iloc[predictions]['product_id'].tolist()\n",
    "\n",
    "        for n in n_values:\n",
    "            hits_at_n[n].append(calculate_hits_at_n(predicted_product_ids, actual_product_id, n))\n",
    "\n",
    "        mrr_scores.append(calculate_mrr(predicted_product_ids, actual_product_id))\n",
    "\n",
    "    hits_at_n_avg = {n: np.mean(scores) for n, scores in hits_at_n.items()}\n",
    "    mrr_avg = np.mean(mrr_scores)\n",
    "\n",
    "    return hits_at_n_avg, mrr_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Hybrid Search**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid search function\n",
    "def hybrid_search(query, k=10, alpha=0.5):\n",
    "    # Semantic search\n",
    "    query_embedding = create_embeddings([query])\n",
    "    D_semantic, I_semantic = index.search(query_embedding.astype('float32'), k * 2)\n",
    "\n",
    "    # TF-IDF search\n",
    "    query_tfidf = tfidf.transform([query])\n",
    "    tfidf_scores = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
    "\n",
    "    # Combine scores\n",
    "    combined_scores = alpha * (1 - D_semantic.flatten() / np.max(D_semantic)) + (1 - alpha) * tfidf_scores[\n",
    "        I_semantic.flatten()]\n",
    "\n",
    "    # Get top k results\n",
    "    top_k_indices = np.argsort(combined_scores)[-k:][::-1]\n",
    "\n",
    "    return I_semantic.flatten()[top_k_indices]\n",
    "\n",
    "# Evaluate hybrid search\n",
    "def evaluate_hybrid_search(df, n_values, alpha=0.5):\n",
    "    hits_at_n = {n: [] for n in n_values}\n",
    "    mrr_scores = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        query = row['query']\n",
    "        actual_product_id = row['product_id']\n",
    "        predictions = hybrid_search(query, k=max(n_values), alpha=alpha)\n",
    "        predicted_product_ids = df_train.iloc[predictions]['product_id'].tolist()\n",
    "\n",
    "        for n in n_values:\n",
    "            hits_at_n[n].append(calculate_hits_at_n(predicted_product_ids, actual_product_id, n))\n",
    "\n",
    "        mrr_scores.append(calculate_mrr(predicted_product_ids, actual_product_id))\n",
    "\n",
    "    hits_at_n_avg = {n: np.mean(scores) for n, scores in hits_at_n.items()}\n",
    "    mrr_avg = np.mean(mrr_scores)\n",
    "\n",
    "    return hits_at_n_avg, mrr_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate All Approaches**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all three approaches\n",
    "print(\"Evaluating semantic search performance...\")\n",
    "n_values = [1, 5, 10]\n",
    "hits_at_n_semantic, mrr_semantic = evaluate_search(df_test, n_values)\n",
    "\n",
    "print(\"Evaluating TF-IDF ranking performance...\")\n",
    "hits_at_n_secondary, mrr_secondary = evaluate_search_with_secondary(df_test, n_values)\n",
    "\n",
    "print(\"Evaluating hybrid search performance...\")\n",
    "hits_at_n_hybrid, mrr_hybrid = evaluate_hybrid_search(df_test, n_values, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize and Save Results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results dictionary\n",
    "results = {\n",
    "    \"Semantic Search\": {\"Hits@N\": hits_at_n_semantic, \"MRR\": mrr_semantic},\n",
    "    \"TF-IDF Ranking\": {\"Hits@N\": hits_at_n_secondary, \"MRR\": mrr_secondary},\n",
    "    \"Hybrid Search\": {\"Hits@N\": hits_at_n_hybrid, \"MRR\": mrr_hybrid}\n",
    "}\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"search_results_{timestamp}.txt\"\n",
    "\n",
    "# Save results to file\n",
    "def save_results_to_file(filename, results_dict):\n",
    "    with open(filename, 'w') as f:\n",
    "        for method, metrics in results_dict.items():\n",
    "            f.write(f\"{method} Results:\\n\")\n",
    "            for metric, value in metrics.items():\n",
    "                if isinstance(value, dict):\n",
    "                    f.write(f\"  {metric}:\\n\")\n",
    "                    for n, score in value.items():\n",
    "                        f.write(f\"    {n}: {score:.4f}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"  {metric}: {value:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "save_results_to_file(filename, results)\n",
    "print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "x = list(hits_at_n_semantic.keys())\n",
    "width = 0.25\n",
    "\n",
    "plt.bar([i - width for i in x], list(hits_at_n_semantic.values()), width, label='Semantic Search')\n",
    "plt.bar(x, list(hits_at_n_secondary.values()), width, label='TF-IDF Ranking')\n",
    "plt.bar([i + width for i in x], list(hits_at_n_hybrid.values()), width, label='Hybrid Search')\n",
    "\n",
    "plt.title('Hits@N Performance Comparison')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.xticks(x)\n",
    "plt.savefig(f'performance_comparison_{timestamp}.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMRR Comparison:\")\n",
    "print(f\"Semantic Search MRR: {mrr_semantic:.4f}\")\n",
    "print(f\"TD-IDF Ranking MRR: {mrr_secondary:.4f}\")\n",
    "print(f\"Hybrid Search MRR: {mrr_hybrid:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save MRR comparison to file\n",
    "with open(filename, 'a') as f:\n",
    "    f.write(\"\\nMRR Comparison:\\n\")\n",
    "    f.write(f\"Semantic Search MRR: {mrr_semantic:.4f}\\n\")\n",
    "    f.write(f\"TD-IDF Ranking MRR: {mrr_secondary:.4f}\\n\")\n",
    "    f.write(f\"Hybrid Search MRR: {mrr_hybrid:.4f}\\n\")\n",
    "\n",
    "print(f\"MRR comparison appended to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
